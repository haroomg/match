{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, validator\n",
    "from dotenv import load_dotenv\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "import fastdup\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Obtenemos los env\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# Crea una instancia del cliente de S3\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matching_images(BaseModel):\n",
    "    \n",
    "    bucket: str\n",
    "    path_origin_file: str\n",
    "    path_alternative_file: str\n",
    "    path_origin_img: str\n",
    "    path_alternative_img: str\n",
    "    img_per_object: Union[int, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/matching/image/\")\n",
    "async def matching_images(matching_data):\n",
    "    bucket = matching_data[\"bucket\"]\n",
    "    path_origin_file = matching_data[\"path_origin_file\"]\n",
    "    path_alternative_file = matching_data[\"path_alternative_file\"]\n",
    "    path_origin_img = matching_data[\"path_origin_img\"]\n",
    "    path_alternative_img = matching_data[\"path_alternative_img\"]\n",
    "    img_per_object = matching_data[\"img_per_object\"]\n",
    "\n",
    "    # validamos que el bucket exista\n",
    "    if bucket:\n",
    "        buckets: list = [bucket[\"Name\"] for bucket in s3.list_buckets()[\"Buckets\"]]\n",
    "        \n",
    "        if bucket not in buckets:\n",
    "            raise HTTPException(status_code=404, detail=f\"El nombre del Bucket '{bucket}' esta mal escrito o no existe.\")\n",
    "    else:\n",
    "        raise HTTPException(status_code=404, detail=\"Debes ingresar el nombre del Bucket\")\n",
    "    del buckets\n",
    "    \n",
    "    # validamos que los archivos origin y alternative existan\n",
    "    for path, type in zip([path_origin_file, path_alternative_file],[\"origin-file\", \"alternative-file\"]):\n",
    "        \n",
    "        if path:\n",
    "            if isinstance(path, str):\n",
    "                \n",
    "                extencion = path.split(\".\")[-1]\n",
    "                \n",
    "                if extencion == \"json\":\n",
    "                    try:\n",
    "                        s3.head_object(Bucket=bucket, Key=path)\n",
    "                    except:\n",
    "                        raise HTTPException(status_code=404, detail=f\"{type}: El archivo '{path}' no existe o esta mal escrito\")\n",
    "                else:\n",
    "                    raise HTTPException(status_code=404, detail=f\"{type}: El archivo '{path}' debe de ser se tipo json no de '{extencion}'\")\n",
    "            else:\n",
    "                raise HTTPException(status_code=404, detail=f\"{type}: El parametro debe de ser de tipo str no de {type(path)}\")\n",
    "        else:\n",
    "            raise HTTPException(status_code=404, detail=f\"{type}: No se puede dejar vacio este atributo.\")\n",
    "    del extencion\n",
    "    \n",
    "    # validamos que la direccion donde ese encuentran las imagenes existan\n",
    "    for path, type in zip([path_origin_img, path_alternative_img],[\"origin-image\", \"alternative-image\"]):\n",
    "        \n",
    "        if path:\n",
    "            response = s3.list_objects_v2(Bucket=bucket, Prefix=path)\n",
    "            if \"Contents\" not in response:\n",
    "                raise HTTPException(status_code=404, detail=f\"{type}: La ruta ingresada no existe o esta mal escrita.\")\n",
    "        else:\n",
    "            raise HTTPException(status_code=404, detail=f\"{type}: Debes ingresar la direccion de las imagenes.\")\n",
    "    del response\n",
    "    \n",
    "    # descargamos los archivos del origin y el aternative\n",
    "    PATH_TRASH: str = \"../trash/s3/json/\"\n",
    "    \n",
    "    file_origin: str = os.path.join(PATH_TRASH, path_origin_file.split(\"/\")[-1])\n",
    "    file_alternative: str = os.path.join(PATH_TRASH, path_alternative_file.split(\"/\")[-1])\n",
    "    \n",
    "    # descargamos los archivos json\n",
    "    for path_local, path_s3 in zip([file_origin, file_alternative], [path_origin_file, path_alternative_file]):\n",
    "        \n",
    "        s3.download_file(\n",
    "            bucket, \n",
    "            path_s3,\n",
    "            path_local\n",
    "        )\n",
    "    \n",
    "    # pasamos los json file a df y despues borramos esos archivos\n",
    "    \n",
    "    df_origin: pd.DataFrame = pd.read_json(file_origin)\n",
    "    os.remove(file_origin)\n",
    "    \n",
    "    df_aternative: pd.DataFrame = pd.read_json(file_alternative)\n",
    "    os.remove(file_alternative)\n",
    "    \n",
    "    WORK_DIR: str = \"../trash/fastdup/\"\n",
    "    FIELD_NAME_IMAGES: str = \"product_images\"\n",
    "    input_dir: list = []\n",
    "    \n",
    "    # abquirimos el nombre de losarchivos para armar un file txt con la ruta deca imagen para pasarlo como argumento al input_dir\n",
    "    list_images_name_origin: list = df_origin[FIELD_NAME_IMAGES].to_list()\n",
    "    list_images_name_alternative: list = df_aternative[FIELD_NAME_IMAGES].to_list()\n",
    "    \n",
    "    for path_s3, list_img in zip([path_origin_img, path_alternative_img], [list_images_name_origin, list_images_name_alternative]):\n",
    "        \n",
    "        for images in list_img: \n",
    "            \n",
    "            if img_per_object == 0:\n",
    "                amount = len(list_img)\n",
    "            else:\n",
    "                if len(list_img) <= img_per_object:\n",
    "                    amount = len(list_img)\n",
    "                if len(list_img) > img_per_object:\n",
    "                    amount = img_per_object\n",
    "                    \n",
    "            for img  in images[0:amount]:\n",
    "                if img:\n",
    "                    input_dir.append(\n",
    "                        f\"s3://{bucket}/{path_s3}{img}\\n\"\n",
    "                    )\n",
    "                \n",
    "    path_files_s3: str = \"../trash/fastdup/address_files_s3.txt\"\n",
    "    with open(path_files_s3, \"w\", encoding=\"utf8\") as file:\n",
    "        for path in input_dir:\n",
    "            file.write(path)\n",
    "        \n",
    "    fd = fastdup.create(WORK_DIR)\n",
    "    fd.run(path_files_s3, threshold= 0.5, overwrite= True, high_accuracy= True)\n",
    "    similarity = fd.similarity()\n",
    "    os.remove(path_files_s3)\n",
    "    \n",
    "    for col_name in [\"filename_from\", \"filename_to\"]:\n",
    "        similarity[col_name] = similarity[col_name].apply(lambda x : x.split(\"/\")[-1])\n",
    "\n",
    "    # aqui empieza el anailisis de la data de cuales fueron las imagenes con similitud\n",
    "\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    # concatenamos los elemontos que se encuentren en filename_from del origin\n",
    "    for i in range(0, len(df_origin)):\n",
    "        \n",
    "        product_images = df_origin[\"product_images\"][i]\n",
    "        search = similarity[similarity[\"filename_from\"].isin(product_images) & ~(similarity[\"filename_to\"].isin(product_images))]\n",
    "        \n",
    "        if len(search) != 0:\n",
    "            \n",
    "            result = pd.concat([result, search])\n",
    "\n",
    "    result = result.reset_index(drop=True)\n",
    "\n",
    "    # eliminamos los elementos que se encuentren en filename_to del df_origin\n",
    "    for i in range(0, len(df_origin)):\n",
    "        \n",
    "        product_images = df_origin[\"product_images\"][i]\n",
    "        search = result[result[\"filename_to\"].isin(product_images)].index\n",
    "            \n",
    "        if len(search) != 0:   \n",
    "            result = result.drop(index=search)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    # return {\n",
    "    #     \"bucket\": bucket,\n",
    "    #     \"path_origin_file\": path_origin_file,\n",
    "    #     \"path_alternative_file\": path_alternative_file,\n",
    "    #     \"path_origin_img\": path_origin_img,\n",
    "    #     \"path_alternative_img\": path_alternative_img,\n",
    "    #     \"img_per_object\": img_per_object\n",
    "    # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = {\n",
    "    \"bucket\": \"hydrahi4ai\",\n",
    "    \"path_origin_file\": \"ajio-myntra/origin/20231214/New_collector_20231214_154733.success.json\",\n",
    "    \"path_alternative_file\": \"ajio-myntra/alternative/20231219/Myntra__Marianfer_Cruz_20231219_195028.success.json\",\n",
    "    \"path_origin_img\": \"ajio-myntra/origin/20231214/\",\n",
    "    \"path_alternative_img\": \"ajio-myntra/alternative/20231219/\",\n",
    "    \"img_per_object\": 0\n",
    "}\n",
    "\n",
    "similarity_test = matching_images(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = pd.read_json(\"../data/json/New_collector_20231214_154733.success.json\")\n",
    "alternative  = pd.read_json(\"../data/json/Myntra__Marianfer_Cruz_20231213_143830.success.json\")\n",
    "\n",
    "for col_name in [\"filename_from\", \"filename_to\"]:\n",
    "    similarity_test[col_name] = similarity_test[col_name].apply(lambda x : x.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui empieza el anailisis de la data de cuales fueron las imagenes con similitud\n",
    "\n",
    "result = pd.DataFrame()\n",
    "\n",
    "# concatenamos los elemontos que se encuentren en filename_from del origin\n",
    "for i in range(0, len(origin)):\n",
    "    \n",
    "    product_images = origin[\"product_images\"][i]\n",
    "    search = similarity_test[similarity_test[\"filename_from\"].isin(product_images) & ~(similarity_test[\"filename_to\"].isin(product_images))]\n",
    "    \n",
    "    if len(search) != 0:\n",
    "        \n",
    "        result = pd.concat([result, search])\n",
    "\n",
    "result = result.reset_index(drop=True)\n",
    "\n",
    "# eliminamos los elementos que se encuentren en filename_to del origin\n",
    "for i in range(0, len(origin)):\n",
    "    \n",
    "    product_images = origin[\"product_images\"][i]\n",
    "    \n",
    "    search = result[result[\"filename_to\"].isin(product_images)].index\n",
    "        \n",
    "    if len(search) != 0:   \n",
    "        result = result.drop(index=search)\n",
    "\n",
    "result = result.reset_index(drop=True)\n",
    "\n",
    "for i in range(0,len(result)):\n",
    "    \n",
    "    filename_origin = result[\"filename_from\"][i]\n",
    "    search_index_origin = lambda lista : filename_origin in lista\n",
    "    index_orgin = origin[\"product_images\"].apply(search_index_origin)\n",
    "    ref_origin = origin.loc[index_orgin, :][\"sku\"]\n",
    "    \n",
    "    filename_alternative = result[\"filename_to\"][i]\n",
    "    search_index_aternative = lambda lista : filename_alternative in lista\n",
    "    index_alternative = alternative[\"product_images\"].apply(search_index_aternative)\n",
    "    ref_alternative = alternative.loc[index_alternative, : ][\"sku\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>distance</th>\n",
       "      <th>filename_from</th>\n",
       "      <th>index_x</th>\n",
       "      <th>error_code_from</th>\n",
       "      <th>is_valid_from</th>\n",
       "      <th>fd_index_from</th>\n",
       "      <th>filename_to</th>\n",
       "      <th>index_y</th>\n",
       "      <th>error_code_to</th>\n",
       "      <th>is_valid_to</th>\n",
       "      <th>fd_index_to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107</td>\n",
       "      <td>938</td>\n",
       "      <td>0.961787</td>\n",
       "      <td>j_lq5dbaxrzw1czqfic.c055587471519cfa3f5265a4ddb4202bf3d91720.file_lq5dc5wi27f1l05yti.[_].jpg</td>\n",
       "      <td>107</td>\n",
       "      <td>VALID</td>\n",
       "      <td>True</td>\n",
       "      <td>107</td>\n",
       "      <td>j_lqcqq8vk11jwo3y4nk.158f76d140a861f1ccc2227cf45917d28ba50000.file_lqcr8x402evb2db4f9.[_].webp</td>\n",
       "      <td>938</td>\n",
       "      <td>VALID</td>\n",
       "      <td>True</td>\n",
       "      <td>938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>167</td>\n",
       "      <td>878</td>\n",
       "      <td>0.971031</td>\n",
       "      <td>j_lq5dbaxrzw1czqfic.c055587471519cfa3f5265a4ddb4202bf3d91720.file_lq5dcenv25h6q9ze75.[_].jpg</td>\n",
       "      <td>167</td>\n",
       "      <td>VALID</td>\n",
       "      <td>True</td>\n",
       "      <td>167</td>\n",
       "      <td>j_lqcqq8vk11jwo3y4nk.c236d247062a358a67be586abfce62926dffa65f.file_lqcr72zs23f1u2nn07.[_].webp</td>\n",
       "      <td>878</td>\n",
       "      <td>VALID</td>\n",
       "      <td>True</td>\n",
       "      <td>878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>501</td>\n",
       "      <td>830</td>\n",
       "      <td>0.750323</td>\n",
       "      <td>j_lq5dbaxrzw1czqfic.c055587471519cfa3f5265a4ddb4202bf3d91720.file_lq5dceo313ucnh0h1g.[_].jpg</td>\n",
       "      <td>501</td>\n",
       "      <td>VALID</td>\n",
       "      <td>True</td>\n",
       "      <td>501</td>\n",
       "      <td>j_lqcqq8vk11jwo3y4nk.d5490618f6563a2efae63455af7404faa1c81e3b.file_lqcr0izpfm0j3q8n0.[_].webp</td>\n",
       "      <td>830</td>\n",
       "      <td>VALID</td>\n",
       "      <td>True</td>\n",
       "      <td>830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500</td>\n",
       "      <td>830</td>\n",
       "      <td>0.736375</td>\n",
       "      <td>j_lq5dbaxrzw1czqfic.c055587471519cfa3f5265a4ddb4202bf3d91720.file_lq5dceo3r1xnx980l.[_].jpg</td>\n",
       "      <td>500</td>\n",
       "      <td>VALID</td>\n",
       "      <td>True</td>\n",
       "      <td>500</td>\n",
       "      <td>j_lqcqq8vk11jwo3y4nk.d5490618f6563a2efae63455af7404faa1c81e3b.file_lqcr0izpfm0j3q8n0.[_].webp</td>\n",
       "      <td>830</td>\n",
       "      <td>VALID</td>\n",
       "      <td>True</td>\n",
       "      <td>830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   from   to  distance                                                                                 filename_from  index_x error_code_from  is_valid_from  fd_index_from                                                                                     filename_to  index_y error_code_to  is_valid_to  fd_index_to\n",
       "0   107  938  0.961787  j_lq5dbaxrzw1czqfic.c055587471519cfa3f5265a4ddb4202bf3d91720.file_lq5dc5wi27f1l05yti.[_].jpg      107           VALID           True            107  j_lqcqq8vk11jwo3y4nk.158f76d140a861f1ccc2227cf45917d28ba50000.file_lqcr8x402evb2db4f9.[_].webp      938         VALID         True          938\n",
       "1   167  878  0.971031  j_lq5dbaxrzw1czqfic.c055587471519cfa3f5265a4ddb4202bf3d91720.file_lq5dcenv25h6q9ze75.[_].jpg      167           VALID           True            167  j_lqcqq8vk11jwo3y4nk.c236d247062a358a67be586abfce62926dffa65f.file_lqcr72zs23f1u2nn07.[_].webp      878         VALID         True          878\n",
       "2   501  830  0.750323  j_lq5dbaxrzw1czqfic.c055587471519cfa3f5265a4ddb4202bf3d91720.file_lq5dceo313ucnh0h1g.[_].jpg      501           VALID           True            501   j_lqcqq8vk11jwo3y4nk.d5490618f6563a2efae63455af7404faa1c81e3b.file_lqcr0izpfm0j3q8n0.[_].webp      830         VALID         True          830\n",
       "3   500  830  0.736375   j_lq5dbaxrzw1czqfic.c055587471519cfa3f5265a4ddb4202bf3d91720.file_lq5dceo3r1xnx980l.[_].jpg      500           VALID           True            500   j_lqcqq8vk11jwo3y4nk.d5490618f6563a2efae63455af7404faa1c81e3b.file_lqcr0izpfm0j3q8n0.[_].webp      830         VALID         True          830"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

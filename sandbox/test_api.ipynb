{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lharo/programing/projects/matching/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, validator\n",
    "from dotenv import load_dotenv\n",
    "from typing import Union\n",
    "import pandas as pd\n",
    "import fastdup\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Obtenemos los env\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# Crea una instancia del cliente de S3\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matching_images(BaseModel):\n",
    "    \n",
    "    bucket: str\n",
    "    path_origin_file: str\n",
    "    path_alternative_file: str\n",
    "    path_origin_img: str\n",
    "    path_alternative_img: str\n",
    "    img_per_object: Union[int, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/matching/image/\")\n",
    "def matching_images(matching_data):\n",
    "    bucket = matching_data[\"bucket\"]\n",
    "    path_origin_file = matching_data[\"path_origin_file\"]\n",
    "    path_alternative_file = matching_data[\"path_alternative_file\"]\n",
    "    path_origin_img = matching_data[\"path_origin_img\"]\n",
    "    path_alternative_img = matching_data[\"path_alternative_img\"]\n",
    "    img_per_object = matching_data[\"img_per_object\"]\n",
    "\n",
    "    # validamos que el bucket exista\n",
    "    if bucket:\n",
    "        buckets: list = [bucket[\"Name\"] for bucket in s3.list_buckets()[\"Buckets\"]]\n",
    "        \n",
    "        if bucket not in buckets:\n",
    "            raise HTTPException(status_code=404, detail=f\"El nombre del Bucket '{bucket}' esta mal escrito o no existe.\")\n",
    "    else:\n",
    "        raise HTTPException(status_code=404, detail=\"Debes ingresar el nombre del Bucket\")\n",
    "    del buckets\n",
    "    \n",
    "    # validamos que los archivos origin y alternative existan\n",
    "    for path, type in zip([path_origin_file, path_alternative_file],[\"origin-file\", \"alternative-file\"]):\n",
    "        \n",
    "        if path:\n",
    "            if isinstance(path, str):\n",
    "                \n",
    "                extencion = path.split(\".\")[-1]\n",
    "                \n",
    "                if extencion == \"json\":\n",
    "                    try:\n",
    "                        s3.head_object(Bucket=bucket, Key=path)\n",
    "                    except:\n",
    "                        raise HTTPException(status_code=404, detail=f\"{type}: El archivo '{path}' no existe o esta mal escrito\")\n",
    "                else:\n",
    "                    raise HTTPException(status_code=404, detail=f\"{type}: El archivo '{path}' debe de ser se tipo json no de '{extencion}'\")\n",
    "            else:\n",
    "                raise HTTPException(status_code=404, detail=f\"{type}: El parametro debe de ser de tipo str no de {type(path)}\")\n",
    "        else:\n",
    "            raise HTTPException(status_code=404, detail=f\"{type}: No se puede dejar vacio este atributo.\")\n",
    "    del extencion\n",
    "    \n",
    "    # validamos que la direccion donde ese encuentran las imagenes existan\n",
    "    for path, type in zip([path_origin_img, path_alternative_img],[\"origin-image\", \"alternative-image\"]):\n",
    "        \n",
    "        if path:\n",
    "            response = s3.list_objects_v2(Bucket=bucket, Prefix=path)\n",
    "            if \"Contents\" not in response:\n",
    "                raise HTTPException(status_code=404, detail=f\"{type}: La ruta ingresada no existe o esta mal escrita.\")\n",
    "        else:\n",
    "            raise HTTPException(status_code=404, detail=f\"{type}: Debes ingresar la direccion de las imagenes.\")\n",
    "    del response\n",
    "    \n",
    "    # descargamos los archivos del origin y el aternative\n",
    "    PATH_TRASH: str = \"../trash/s3/json/\"\n",
    "    \n",
    "    file_origin: str = os.path.join(PATH_TRASH, path_origin_file.split(\"/\")[-1])\n",
    "    file_alternative: str = os.path.join(PATH_TRASH, path_alternative_file.split(\"/\")[-1])\n",
    "    \n",
    "    # descargamos los archivos json\n",
    "    for path_local, path_s3 in zip([file_origin, file_alternative], [path_origin_file, path_alternative_file]):\n",
    "        \n",
    "        s3.download_file(\n",
    "            bucket, \n",
    "            path_s3,\n",
    "            path_local\n",
    "        )\n",
    "    \n",
    "    # pasamos los json file a df y despues borramos esos archivos\n",
    "    \n",
    "    df_origin: pd.DataFrame = pd.read_json(file_origin)\n",
    "    os.remove(file_origin)\n",
    "    \n",
    "    df_aternative: pd.DataFrame = pd.read_json(file_alternative)\n",
    "    os.remove(file_alternative)\n",
    "    \n",
    "    WORK_DIR: str = \"../trash/fastdup/\"\n",
    "    FIELD_NAME_IMAGES: str = \"product_images\"\n",
    "    input_dir: list = []\n",
    "    \n",
    "    # abquirimos el nombre de losarchivos para armar un file txt con la ruta deca imagen para pasarlo como argumento al input_dir\n",
    "    list_images_name_origin: list = df_origin[FIELD_NAME_IMAGES].to_list()\n",
    "    list_images_name_alternative: list = df_aternative[FIELD_NAME_IMAGES].to_list()\n",
    "    \n",
    "    for path_s3, list_img in zip([path_origin_img, path_alternative_img], [list_images_name_origin, list_images_name_alternative]):\n",
    "        \n",
    "        for images in list_img: \n",
    "            \n",
    "            if img_per_object == 0:\n",
    "                amount = len(list_img)\n",
    "            else:\n",
    "                if len(list_img) <= img_per_object:\n",
    "                    amount = len(list_img)\n",
    "                if len(list_img) > img_per_object:\n",
    "                    amount = img_per_object\n",
    "                    \n",
    "            for img  in images[0:amount]:\n",
    "                if img:\n",
    "                    input_dir.append(\n",
    "                        f\"s3://{bucket}/{path_s3}{img}\\n\"\n",
    "                    )\n",
    "                \n",
    "    path_files_s3: str = \"../trash/fastdup/address_files_s3.txt\"\n",
    "    with open(path_files_s3, \"w\", encoding=\"utf8\") as file:\n",
    "        for path in input_dir:\n",
    "            file.write(path)\n",
    "        \n",
    "    fd = fastdup.create(WORK_DIR)\n",
    "    fd.run(path_files_s3, threshold= 0.5, overwrite= True, high_accuracy= True)\n",
    "    similarity = fd.similarity()\n",
    "    os.remove(path_files_s3)\n",
    "    \n",
    "    for col_name in [\"filename_from\", \"filename_to\"]:\n",
    "        similarity[col_name] = similarity[col_name].apply(lambda x : x.split(\"/\")[-1])\n",
    "\n",
    "    # aqui empieza el anailisis de la data de cuales fueron las imagenes con similitud\n",
    "\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    # concatenamos los elemontos que se encuentren en filename_from del origin\n",
    "    for i in range(0, len(df_origin)):\n",
    "        \n",
    "        product_images = df_origin[\"product_images\"][i]\n",
    "        search = similarity[similarity[\"filename_from\"].isin(product_images) & ~(similarity[\"filename_to\"].isin(product_images))]\n",
    "        \n",
    "        if len(search) != 0:\n",
    "            \n",
    "            result = pd.concat([result, search])\n",
    "\n",
    "    result = result.reset_index(drop=True)\n",
    "\n",
    "    # eliminamos los elementos que se encuentren en filename_to del df_origin\n",
    "    for i in range(0, len(df_origin)):\n",
    "        \n",
    "        product_images = df_origin[\"product_images\"][i]\n",
    "        search = result[result[\"filename_to\"].isin(product_images)].index\n",
    "            \n",
    "        if len(search) != 0:   \n",
    "            result = result.drop(index=search)\n",
    "    \n",
    "    return result\n",
    "    \n",
    "    # return {\n",
    "    #     \"bucket\": bucket,\n",
    "    #     \"path_origin_file\": path_origin_file,\n",
    "    #     \"path_alternative_file\": path_alternative_file,\n",
    "    #     \"path_origin_img\": path_origin_img,\n",
    "    #     \"path_alternative_img\": path_alternative_img,\n",
    "    #     \"img_per_object\": img_per_object\n",
    "    # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = {\n",
    "    \"bucket\": \"hydrahi4ai\",\n",
    "    \"path_origin_file\": \"ajio-myntra/origin/20231214/New_collector_20231214_154733.success.json\",\n",
    "    \"path_alternative_file\": \"ajio-myntra/alternative/20231213/Myntra__Marianfer_Cruz_20231213_143830.success.json\",\n",
    "    \"path_origin_img\": \"ajio-myntra/origin/20231214/\",\n",
    "    \"path_alternative_img\": \"ajio-myntra/alternative/20231213/\",\n",
    "    \"img_per_object\": 1\n",
    "}\n",
    "\n",
    "similarity_test = matching_images(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = pd.read_json(\"../data/json/New_collector_20231214_154733.success.json\")\n",
    "alternative  = pd.read_json(\"../data/json/Myntra__Marianfer_Cruz_20231213_143830.success.json\")\n",
    "\n",
    "for col_name in [\"filename_from\", \"filename_to\"]:\n",
    "    similarity_test[col_name] = similarity_test[col_name].apply(lambda x : x.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464087281009 19840254\n"
     ]
    }
   ],
   "source": [
    "# aqui empieza el anailisis de la data de cuales fueron las imagenes con similitud\n",
    "\n",
    "result = pd.DataFrame()\n",
    "\n",
    "# concatenamos los elemontos que se encuentren en filename_from del origin\n",
    "for i in range(0, len(origin)):\n",
    "    \n",
    "    product_images = origin[\"product_images\"][i]\n",
    "    search = similarity_test[similarity_test[\"filename_from\"].isin(product_images) & ~(similarity_test[\"filename_to\"].isin(product_images))]\n",
    "    \n",
    "    if len(search) != 0:\n",
    "        \n",
    "        result = pd.concat([result, search])\n",
    "\n",
    "result = result.reset_index(drop=True)\n",
    "\n",
    "# eliminamos los elementos que se encuentren en filename_to del origin\n",
    "for i in range(0, len(origin)):\n",
    "    \n",
    "    product_images = origin[\"product_images\"][i]\n",
    "    \n",
    "    search = result[result[\"filename_to\"].isin(product_images)].index\n",
    "        \n",
    "    if len(search) != 0:   \n",
    "        result = result.drop(index=search)\n",
    "\n",
    "result = result.reset_index(drop=True)\n",
    "\n",
    "\n",
    "for i in range(0,len(result)):\n",
    "    \n",
    "    filename_origin = result[\"filename_from\"][i]\n",
    "    search_index_origin = lambda lista : filename_origin in lista\n",
    "    index_orgin = (origin[\"product_images\"].apply(search_index_origin)).index\n",
    "    ref_origin = origin[\"sku\"][index_orgin[0]]\n",
    "    \n",
    "    filename_alternative = result[\"filename_to\"][i]\n",
    "    search_index_aternative = lambda lista : filename_alternative in lista\n",
    "    index_alternative = (alternative[\"product_images\"].apply(search_index_aternative)).index\n",
    "    ref_alternative = alternative[\"sku\"][index_alternative[0]]\n",
    "\n",
    "    break\n",
    "    \n",
    "print(ref_origin, ref_alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([11, 13, 18, 19], dtype='int64')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

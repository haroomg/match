{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from typing import Union\n",
    "import pandas as pd \n",
    "import fastdup\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Obtenemos los env\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# Crea una instancia del cliente de S3\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    ")\n",
    "\n",
    "class Matching_images(BaseModel):\n",
    "    \n",
    "    bucket: str\n",
    "    path_origin_file: str\n",
    "    path_alternative_file: str\n",
    "    path_origin_img: str\n",
    "    path_alternative_img: str\n",
    "    path_report_matching_similar: str\n",
    "    img_per_object: Union[int, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/matching/image/\")\n",
    "def matching_images(matching_data):\n",
    "    bucket = matching_data[\"bucket\"]\n",
    "    path_origin_file = matching_data[\"path_origin_file\"]\n",
    "    path_alternative_file = matching_data[\"path_alternative_file\"]\n",
    "    path_origin_img = matching_data[\"path_origin_img\"]\n",
    "    path_alternative_img = matching_data[\"path_alternative_img\"]\n",
    "    path_report = matching_data[\"path_report\"]\n",
    "    img_per_object = matching_data[\"img_per_object\"]\n",
    "\n",
    "    # validamos que el bucket exista\n",
    "    if bucket:\n",
    "        buckets: list = [bucket[\"Name\"] for bucket in s3.list_buckets()[\"Buckets\"]]\n",
    "        \n",
    "        if bucket not in buckets:\n",
    "            raise HTTPException(status_code=404, detail=f\"El nombre del Bucket '{bucket}' esta mal escrito o no existe.\")\n",
    "    else:\n",
    "        raise HTTPException(status_code=404, detail=\"Debes ingresar el nombre del Bucket\")\n",
    "    del buckets\n",
    "    \n",
    "    # validamos que los archivos origin y alternative existan\n",
    "    for path, type in zip([path_origin_file, path_alternative_file],[\"origin-file\", \"alternative-file\"]):\n",
    "        \n",
    "        if path:\n",
    "            if isinstance(path, str):\n",
    "                \n",
    "                extencion = path.split(\".\")[-1]\n",
    "                \n",
    "                if extencion == \"json\":\n",
    "                    try:\n",
    "                        s3.head_object(Bucket=bucket, Key=path)\n",
    "                    except:\n",
    "                        raise HTTPException(status_code=404, detail=f\"{type}: El archivo '{path}' no existe o esta mal escrito\")\n",
    "                else:\n",
    "                    raise HTTPException(status_code=404, detail=f\"{type}: El archivo '{path}' debe de ser se tipo json no de '{extencion}'\")\n",
    "            else:\n",
    "                raise HTTPException(status_code=404, detail=f\"{type}: El parametro debe de ser de tipo str no de {type(path)}\")\n",
    "        else:\n",
    "            raise HTTPException(status_code=404, detail=f\"{type}: No se puede dejar vacio este atributo.\")\n",
    "    del extencion\n",
    "    \n",
    "    # validamos que la direccion donde ese encuentran las imagenes existan\n",
    "    for path, type in zip([path_origin_img, path_alternative_img],[\"origin-image\", \"alternative-image\"]):\n",
    "        \n",
    "        if path:\n",
    "            response = s3.list_objects_v2(Bucket=bucket, Prefix=path)\n",
    "            if \"Contents\" not in response:\n",
    "                raise HTTPException(status_code=404, detail=f\"{type}: La ruta ingresada no existe o esta mal escrita.\")\n",
    "        else:\n",
    "            raise HTTPException(status_code=404, detail=f\"{type}: Debes ingresar la direccion de las imagenes.\")\n",
    "    del response\n",
    "    \n",
    "    # validamos que la direccion donde se va a guardar el reporte exista y que el nombre del archivo no este siendo usado por otro\n",
    "    if path_report:\n",
    "        \n",
    "        # validamos que exista la carpeta\n",
    "        path = \"/\".join(path_report.split(\"/\")[:-1])\n",
    "        \n",
    "        response = s3.list_objects_v2(Bucket=bucket, Prefix=path)\n",
    "        if \"Contents\" not in response:\n",
    "                raise HTTPException(status_code=404, detail=f\"{type}: La ruta ingresada donde se va a guardar el archivo no existe o esta mal escrita.\")\n",
    "        \n",
    "        # validamos que el nombre del archivo dado no este siendo usado por otro archivo\n",
    "        file_name = path_report.split(\"/\")[-1]\n",
    "        \n",
    "        if file_name.split(\".\")[-1] == \"json\":\n",
    "            try:\n",
    "                s3.head_object(Bucket=bucket, Key=path)\n",
    "                raise HTTPException(f\"El archivo {file_name} ya existe en {path}.\")\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            raise HTTPException(status_code=404, detail=f\"La extencion del reporte debe de ser de tipo json.\")\n",
    "        \n",
    "    else:\n",
    "        raise HTTPException(status_code=404, detail=\"Debes ingresar la direccion donde se va a guardar el reporte\")\n",
    "    \n",
    "    # descargamos los archivos del origin y el aternative\n",
    "    PATH_TRASH: str = \"../trash/s3/\"\n",
    "    \n",
    "    file_origin: str = os.path.join(PATH_TRASH, path_origin_file.split(\"/\")[-1])\n",
    "    file_alternative: str = os.path.join(PATH_TRASH, path_alternative_file.split(\"/\")[-1])\n",
    "    \n",
    "    # descargamos los archivos json\n",
    "    for path_local, path_s3 in zip([file_origin, file_alternative], [path_origin_file, path_alternative_file]):\n",
    "        \n",
    "        s3.download_file(\n",
    "            bucket, \n",
    "            path_s3,\n",
    "            path_local\n",
    "        )\n",
    "    \n",
    "    # pasamos los json file a df y despues borramos esos archivos\n",
    "    \n",
    "    df_origin: pd.DataFrame = pd.read_json(file_origin)\n",
    "    os.remove(file_origin)\n",
    "    \n",
    "    df_aternative: pd.DataFrame = pd.read_json(file_alternative)\n",
    "    os.remove(file_alternative)\n",
    "    \n",
    "    WORK_DIR: str = \"../trash/fastdup/\"\n",
    "    FIELD_NAME_IMAGES: str = \"product_images\"\n",
    "    input_dir: list = []\n",
    "    \n",
    "    # abquirimos el nombre de los archivos para armar un file txt con la ruta de cada imagen para pasarlo como argumento al input_dir\n",
    "    list_images_name_origin: list = df_origin[FIELD_NAME_IMAGES].to_list()\n",
    "    list_images_name_alternative: list = df_aternative[FIELD_NAME_IMAGES].to_list()\n",
    "    \n",
    "    for path_s3, list_img in zip([path_origin_img, path_alternative_img], [list_images_name_origin, list_images_name_alternative]):\n",
    "        \n",
    "        for images in list_img: \n",
    "            \n",
    "            if img_per_object == 0:\n",
    "                amount = len(list_img)\n",
    "            else:\n",
    "                if len(list_img) <= img_per_object:\n",
    "                    amount = len(list_img)\n",
    "                if len(list_img) > img_per_object:\n",
    "                    amount = img_per_object\n",
    "                    \n",
    "            for img  in images[0:amount]:\n",
    "                if img:\n",
    "                    input_dir.append(\n",
    "                        f\"s3://{bucket}/{path_s3}{img}\\n\"\n",
    "                    )\n",
    "                \n",
    "    path_files_s3: str = \"../trash/fastdup/address_files_s3.txt\"\n",
    "    with open(path_files_s3, \"w\", encoding=\"utf8\") as file:\n",
    "        for path in input_dir:\n",
    "            file.write(path)\n",
    "        \n",
    "    fd = fastdup.create(WORK_DIR)\n",
    "    fd.run(path_files_s3, threshold= 0.5, overwrite= True, high_accuracy= True)\n",
    "    similarity = fd.similarity()\n",
    "    os.remove(path_files_s3)\n",
    "    \n",
    "    for col_name in [\"filename_from\", \"filename_to\"]:\n",
    "        similarity[col_name] = similarity[col_name].apply(lambda x : x.split(\"/\")[-1])\n",
    "\n",
    "    # aqui empieza el anailisis de la data de cuales fueron las imagenes con similitud\n",
    "    result = pd.DataFrame()\n",
    "    matches: dict = {}\n",
    "\n",
    "    # concatenamos los elemontos que se encuentren en filename_from del origin\n",
    "    for index, row in df_origin.iterrows():\n",
    "        \n",
    "        product_images = row[\"product_images\"]\n",
    "        search = similarity[similarity[\"filename_from\"].isin(product_images) & ~(similarity[\"filename_to\"].isin(product_images))]\n",
    "        \n",
    "        if len(search) != 0:\n",
    "            \n",
    "            result = pd.concat([result, search])\n",
    "\n",
    "    result = result.reset_index(drop=True)\n",
    "\n",
    "    # eliminamos los elementos que se encuentren en filename_to del origin\n",
    "    for index, row in df_origin.iterrows():\n",
    "        \n",
    "        product_images = row[\"product_images\"]\n",
    "        \n",
    "        search = result[result[\"filename_to\"].isin(product_images)].index\n",
    "            \n",
    "        if len(search) != 0:   \n",
    "            result = result.drop(index=search)\n",
    "\n",
    "    result = result.reset_index(drop=True)\n",
    "\n",
    "    for index, row in result.iterrows():\n",
    "        \n",
    "        # buscamos el sku que corresponde al archivo\n",
    "        \n",
    "        filename_origin = row[\"filename_from\"]\n",
    "        filename_alternative = row[\"filename_to\"]\n",
    "        \n",
    "        for index, row in df_origin.iterrows():\n",
    "            if filename_origin in row[\"product_images\"]:\n",
    "                ref_origin = row[\"sku\"]\n",
    "                break\n",
    "        \n",
    "        for index, row in df_aternative.iterrows():\n",
    "            if filename_alternative in row[\"product_images\"]:\n",
    "                ref_alternative = row[\"sku\"]\n",
    "                break\n",
    "        \n",
    "        if ref_origin not in matches:\n",
    "            matches[ref_origin] = ref_alternative\n",
    "    \n",
    "    local_path: str = \"../trash/reports/\" + path_report.split(\"/\")[-1]\n",
    "    \n",
    "    # creamos un archivo json donde guardaremos el match\n",
    "    with open(local_path, \"w\", encoding=\"utf8\") as file:\n",
    "        json.dump(matches, file, indent=4)\n",
    "    \n",
    "    # subimos el reporte al s3\n",
    "    s3.upload_file(local_path, bucket, path_report)\n",
    "    \n",
    "    # borramos el archivo local \n",
    "    os.remove(local_path)\n",
    "    \n",
    "    return {\n",
    "        \"matches_found\": len(matches),\n",
    "        \"route_where_is_report\": path_report,\n",
    "        \"report_file_name\": path_report.split(\"/\")[-1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = {\n",
    "    \"bucket\": \"hydrahi4ai\",\n",
    "    \"path_origin_file\": \"ajio-myntra/origin/20231214/New_collector_20231214_154733.success.json\",\n",
    "    \"path_alternative_file\": \"ajio-myntra/alternative/20231219/Myntra__Marianfer_Cruz_20231219_195028.success.json\",\n",
    "    \"path_origin_img\": \"ajio-myntra/origin/20231214/\",\n",
    "    \"path_alternative_img\": \"ajio-myntra/alternative/20231219/\",\n",
    "    \"path_report\": \"ajio-myntra/reports/test.json\",\n",
    "    \"img_per_object\": 0\n",
    "}\n",
    "\n",
    "similarity_test = matching_images(request)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.9.5"
=======
   "version": "3.9.10"
>>>>>>> 5a96d28ef6ea2869231e8cffdf341a2153e53cdd
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
